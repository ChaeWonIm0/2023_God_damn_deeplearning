{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPL15KQmqbNyhb4LQAoBkH9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dla9944/God_damn_deeplearning/blob/master/Seq2Seq_sample_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fDtt1Pxn4gtT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Seq2Seq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyWdaUER86Ql",
        "outputId": "a328e38b-fec9-4813-8230-bf64bf4d7217"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Seq2Seq\n",
            "  Downloading seq2seq-0.0.5.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy==1.16.3\n",
            "  Downloading numpy-1.16.3.zip (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting Seq2Seq\n",
            "  Downloading seq2seq-0.0.4.tar.gz (2.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: Seq2Seq\n",
            "  Building wheel for Seq2Seq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Seq2Seq: filename=seq2seq-0.0.4-py2.py3-none-any.whl size=2031 sha256=c69466368ef1d0a0a12d6b3ecce62e9c793321ab2be0b83cbdce076c138a7c8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/e3/d6/6a773e18c55ec3c577f6ad52d28448806632bd859079f401d8\n",
            "Successfully built Seq2Seq\n",
            "Installing collected packages: Seq2Seq\n",
            "Successfully installed Seq2Seq-0.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 256\n",
        "x_ = list(map(ord, 'hello'))\n",
        "y_ = list(map(ord, 'hola'))\n",
        "x = torch.LongTensor(x_)\n",
        "y = torch.LongTensor(y_)"
      ],
      "metadata": {
        "id": "li-thrwy4qVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, vocab_size, hidden_size):\n",
        "      super(Seq2Seq, self).__init__()\n",
        "      self.n_layers = 1\n",
        "      self.hidden_size = hidden_size\n",
        "      self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "      self.encoder = nn.GRU(hidden_size, hidden_size)\n",
        "      self.decoder = nn.GRU(hidden_size, hidden_size)\n",
        "      self.project = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "  def forward(self, inputs, targets):\n",
        "      initial_state = self.__init__state()\n",
        "      embedding = self.embedding(inputs).unsqueeze(1)\n",
        "      encoder_output, encoder_state = self.encoder(embedding, initial_state)\n",
        "      decoder_state = encoder_state\n",
        "      decoder_input = torch.LongTensor([0])\n",
        "\n",
        "      outputs = []\n",
        "      for i in range(targets.size()[0]):\n",
        "          decoder_input = self.embedding(decoder_input).unsqueeze(1)\n",
        "          decoder_output, decoder_state = self.decoder(decoder_input, decoder_state)\n",
        "\n",
        "          projection = self.project(decoder_output)\n",
        "          outputs.append(projection)\n",
        "\n",
        "          decoder_input = torch.LongTensor([targets[i]])\n",
        "      outputs = torch.stack(outputs).squeeze()\n",
        "      return outputs\n",
        "\n",
        "      def _init_state(self, batch_size =1):\n",
        "          weight = next(self.parameters()).data\n",
        "          return weight.new(self.n_layers, batch_size, self.hidden_size).zero_() "
      ],
      "metadata": {
        "id": "HpNeUrmK5GOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq = Seq2Seq(vocab_size, 16)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(seq2seq.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "rKgcmA5g5WYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log = []\n",
        "for i in range(1000):\n",
        "    prediction = seq2seq(x,y)\n",
        "    loss = criterion(prediction, y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss_val = loss.data\n",
        "    log.append(loss_val)\n",
        "    if i % 100 == 0:\n",
        "       print('\\n 반복 : %d 오차 : %s' % (i, loss_val.item()))\n",
        "       _, top1 = prediction.data.topk(1,1)\n",
        "       print([chr(c) for c in top1.squeeze().numpy().tolist()])\n",
        "\n",
        "plt.plot(log)\n",
        "plt.ylabel('cross entropy loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ljMfNc_v5pOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8JS4I-uS76W3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}